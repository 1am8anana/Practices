{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Decision Tree? ###\n",
    "\n",
    "Decision trees are machine learning models that try to find patterns in the features of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='tree_gif.gif' width=\"800\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we’re given this magic tree, it seems relatively easy to make classifications. But how do these trees get created in the first place? Decision trees are supervised machine learning models, which means that they’re created from a training set of labeled data. Creating the tree is where the learning in machine learning happens.\n",
    "\n",
    "Take a look at the gif on this page. We begin with every point in the training set at the top of the tree. These training points have labels — the red points represent students that didn’t get an A on a test and the green points represent students that did get an A on a test.\n",
    "\n",
    "We then decide to split the data into smaller groups based on a feature. For example, that feature could be something like their average grade in the class. Students with an A average would go into one set, students with a B average would go into another subset, and so on.\n",
    "\n",
    "Once we have these subsets, we repeat the process — we split the data in each subset again on a different feature. Eventually, we reach a point where we decide to stop splitting the data into smaller groups. We’ve reached a leaf of the tree. We can now count up the labels of the data in that leaf. If an unlabeled point reaches that leaf, it will be classified as the majority label.\n",
    "\n",
    "We can now make a tree, but how did we know which features to split the data set with? After all, if we started by splitting the data based on the number of hours they slept the night before the test, we’d end up with a very different tree that would produce very different results. How do we know which tree is best? We’ll tackle this question soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Decision Tree ###\n",
    "\n",
    "To answer the questions posed in the previous exercise, we’re going to do things a bit differently in this lesson and work “backwards” (!!!): we’re going to first fit a decision tree to a dataset and visualize this tree using scikit-learn. We’re then going to systematically unpack the following: how to interpret the tree visualization, how scikit-learn‘s implementation works, what is gini impurity, what are parameters and hyper-parameters of the decision tree model, etc.\n",
    "\n",
    "We’re going to use a dataset about cars with six features:\n",
    "* The price of the car, buying, which can be “vhigh”, “high”, “med”, or “low”.\n",
    "* The cost of maintaining the car, maint, which can be “vhigh”, “high”, “med”, or “low”.\n",
    "* The number of doors, doors, which can be “2”, “3”, “4”, “5more”.\n",
    "* The number of people the car can hold, persons, which can be “2”, “4”, or “more”.\n",
    "* The size of the trunk, lugboot, which can be “small”, “med”, or “big”.\n",
    "* The safety rating of the car, safety, which can be “low”, “med”, or “high”.\n",
    "\n",
    "The question we will be trying to answer using decision trees is: when considering buying a car, what factors go into making that decision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Import models from scikit learn module:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "#Loading the dataset\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We’ve imported the dataset in the workspace.\n",
    "\n",
    "* Take a look at the first five rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  buying  maint doors persons lug_boot safety  accep\n",
      "0  vhigh  vhigh     2       2    small    low  unacc\n",
      "1  vhigh  vhigh     2       2    small    med  unacc\n",
      "2  vhigh  vhigh     2       2    small   high  unacc\n",
      "3  vhigh  vhigh     2       2      med    low  unacc\n",
      "4  vhigh  vhigh     2       2      med    med  unacc\n"
     ]
    }
   ],
   "source": [
    "## 1a. Take a look at the dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We’ve created dummy features for the categorical values and set the predictor and target variables as X and y respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1b. Setting the target and predictor variables\n",
    "df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable\n",
    "X = pd.get_dummies(df.iloc[:,0:6])\n",
    "y = df['accep']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can examine the new set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['buying_high', 'buying_low', 'buying_med', 'buying_vhigh', 'maint_high',\n",
      "       'maint_low', 'maint_med', 'maint_vhigh', 'doors_2', 'doors_3',\n",
      "       'doors_4', 'doors_5more', 'persons_2', 'persons_4', 'persons_more',\n",
      "       'lug_boot_big', 'lug_boot_med', 'lug_boot_small', 'safety_high',\n",
      "       'safety_low', 'safety_med'],\n",
      "      dtype='object')\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "## 1c. Examine the new features\n",
    "print(X.columns)\n",
    "print(len(X.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We can now perform a train-test split and fit a decision tree to our training data. We’ll be using scikit-learn‘s train_test_split function to do the split and the DecisionTreeClassifier() class to fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(ccp_alpha=0.01, max_depth=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(ccp_alpha=0.01, max_depth=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.01, max_depth=3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2a. Performing the train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.2)\n",
    "\n",
    "## 2b.Fitting the decision tree classifier\n",
    "dt = DecisionTreeClassifier(max_depth=3, ccp_alpha=0.01,criterion='gini')\n",
    "dt.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We’re now ready to visualize the decision tree! The tree module within scikit-learn has a plotting functionality that allows us to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.Plotting the Tree\n",
    "plt.figure(figsize=(20,12))\n",
    "tree.plot_tree(dt, feature_names = x_train.columns, max_depth=5, class_names = ['unacc', 'acc'], label='all', filled=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "## this code will visualization of the decision tree, But for some reason this code cannnot run in this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting a Decision Tree ###\n",
    "\n",
    "We’re now going to examine the decision tree we built for the car dataset.\n",
    "Two important concepts to note here are the following:\n",
    "\n",
    "1. The root node is identified as the top of the tree. This is notated already with the number of samples and the numbers in each class (i.e. unacceptable vs. acceptable) that was used to build the tree.\n",
    "\n",
    "2. Splits occur with True to the left, False to the right. Note the right split is a leaf node i.e., there are no more branches. Any decision ending here results in the majority class. (The majority class here is unacc.)\n",
    "\n",
    "To interpret the tree, it’s useful to keep in mind that the variables we’re looking at are categorical variables that correspond to:\n",
    "\n",
    "* buying: The price of the car which can be “vhigh”, “high”, “med”, or “low”.\n",
    "* maint: The cost of maintaining the car which can be “vhigh”, “high”, “med”, or “low”.\n",
    "* doors: The number of doors which can be “2”, “3”, “4”, “5more”.\n",
    "* persons: The number of people the car can hold which can be “2”, “4”, or “more”.\n",
    "* lugboot: The size of the trunk which can be “small”, “med”, or “big”.\n",
    "* safety: The safety rating of the car which can be “low”, “med”, or “high”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Impurity ###\n",
    "\n",
    "Consider the two trees below. Which tree would be more useful as a model that tries to predict whether someone would get an A in a class?\n",
    "\n",
    "<img src='comparision.jpeg' width=\"800\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say you use the top tree. You’ll end up at a leaf node where the label is up for debate. The training data has labels from both classes! If you use the bottom tree, you’ll end up at a leaf where there’s only one type of label. There’s no debate at all! We’d be much more confident about our classification if we used the bottom tree.\n",
    "\n",
    "This idea can be quantified by calculating the Gini impurity of a set of data points. For two classes (1 and 2) with probabilites p_1 and p_2 respectively, the Gini impurity is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ 1-(p_{1}^2+p_{2}^2) = (p_{1}^{2}+(1-p_{1})^2) $$\n",
    "\n",
    "<img src='gini.jpeg' width=\"800\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of a decision tree model is to separate the classes the best possible, i.e. minimize the impurity (or maximize the purity). Notice that if p_1 is 0 or 1, the Gini impurity is 0, which means there is only one class so there is perfect separation. From the graph, the Gini impurity is maximum at p_1=0.5, which means the two classes are equally balanced, so this is perfectly impure!\n",
    "\n",
    "In general, the Gini impurity for C classes is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ 1-\\sum \\limits_{1}^{c}p_{i}^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain ###\n",
    "\n",
    "We know that we want to end up with leaves with a low Gini Impurity, but we still need to figure out which features to split on in order to achieve this. To answer this question, we can calculate the information gain of splitting the data on a certain feature. Information gain measures the difference in the impurity of the data before and after the split.\n",
    "\n",
    "For example, let’s start with the root node of our car acceptability tree:\n",
    "\n",
    "<img src='informationgain.png' width=\"800\" height=\"800\">\n",
    "\n",
    "The initial Gini impurity (which we confirmed previously) is 0.418. The first split occurs based on the feature safety_low<=0.5, and as this is a dummy variable with values 0 and 1, this split is pushing higher safety cars to the left (912 samples) and low safety cars to the right (470 samples). Before we discuss how we decided to split on this feature, let’s calculate the information gain.\n",
    "\n",
    "The new Gini impurities for these two split nodes are 0.495 and 0 (which is a pure leaf node!). All together, the now weighted Gini impurity after the split is:\n",
    "\n",
    "$$ 912/1382*(0.495)+470/1382*(0) = 0.3267 $$\n",
    "\n",
    "Not bad! (Remember we want our Gini impurity to be lower!) This is lower than our initial Gini impurity, so by splitting the data in that way, we’ve gained some information about how the data is structured — the datasets after the split are purer than they were before the split.\n",
    "\n",
    "Then the information gain (or reduction in impurity after the split) is\n",
    "\n",
    "$$ 0.4185-0.3267 = 0.0918 $$\n",
    "\n",
    "The higher the information gain the better — if information gain is 0, then splitting the data on that feature was useless!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How a Decision Tree is Built (Feature Split) ####\n",
    "\n",
    "We’re ready to understand how the decision tree was built by scikit-learn. To recap:\n",
    "\n",
    "* The root node in the tree we’ve been using so far is split on the feature safety_low. When its value is 1, this corresponds to the right split (vehicles with low safety) and when its value is 0, this corresponds to the left split.\n",
    "\n",
    "* Information gain is the difference in the weighted gini impurity before and after performing a split at a node. We saw in the previous exercise that information gain for safety_low as root node was 0.4185 - 0.3267 = 0.0918.\n",
    "\n",
    "We now consider an important question: How does one know that this is the best node to split on?! To figure this out we’re going to go through the process of calculating information gain for other possible root node choices and calculate the information gain values for each of these. This is precisely what is going on under the hood when one runs a DecisionTreeClassifier() in scikit-learn. By checking information gain values of all possible options at any given split, the algorithm decide on the best feature to split on at every node.\n",
    "\n",
    "For starters, we will consider a different feature we could have split on first: persons_2. Recall that persons_2 can take a binary value of 0 or 1 as well. Setting persons_2 as the root node means that:\n",
    "\n",
    "* the left split will contain data corresponding to a persons_2 value <0.5.\n",
    "* the right split will contain data corresponding to a persons_2 value >0.5.\n",
    "\n",
    "We’ve defined two functions in the code editor to help us calculate gini impurity (gini) and information gain (info_gain) in the code editor. Read through the functions to see if they reflect the formulas we’ve covered thusfar. Using these functions, we’re going to calculate the information gain from splitting on persons_2. We can then follow this procedure for all other possible root node choices and truly check if safety_low is indeed our best choice for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The usual libraries, loading the dataset and performing the train-test split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])\n",
    "df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable\n",
    "X = pd.get_dummies(df.iloc[:,0:6])\n",
    "y = df['accep']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.2)\n",
    "\n",
    "## Functions to calculate gini impurity and information gain\n",
    "\n",
    "def gini(data):\n",
    "    \"\"\"calculate the Gini Impurity\n",
    "    \"\"\"\n",
    "    data = pd.Series(data)\n",
    "    return 1 - sum(data.value_counts(normalize=True)**2)\n",
    "   \n",
    "def info_gain(left, right, current_impurity):\n",
    "    \"\"\"Information Gain associated with creating a node/split data.\n",
    "    Input: left, right are data in left branch, right branch, respectively\n",
    "    current_impurity is the data impurity before splitting into left, right branches\n",
    "    \"\"\"\n",
    "    # weight for gini score of the left branch\n",
    "    w = float(len(left)) / (len(left) + len(right))\n",
    "    return current_impurity - w * gini(left) - (1 - w) * gini(right)\n",
    "\n",
    "#### -----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create two DataFrames left and right that represent the y_train values that correspond to x_train['persons_2'] being 0 and 1 respectively. Calculate the length of these DataFrames, store them as len_left and len_right, and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of cars with persons_2 == 0: 917\n",
      "No. of cars with persons_2 == 1: 465\n"
     ]
    }
   ],
   "source": [
    "## 1. Calculate sample sizes for a split on `persons_2`\n",
    "left = pd.DataFrame(y_train[x_train['persons_2'] == 0])\n",
    "right = pd.DataFrame(y_train[x_train['persons_2'] == 1])\n",
    "len_left = len(left)\n",
    "len_right = len(right)\n",
    "print ('No. of cars with persons_2 == 0:', len_left)\n",
    "print ('No. of cars with persons_2 == 1:', len_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We’re now going to calculate the gini impurities corresponding to the overall training data and the left and right split. To do so:\n",
    "\n",
    "    1. Calculating the gini impurity in the overall training data.\n",
    "    2. For the gini value of the left node, create a variable gini_left and use the gini function to calculate the value.\n",
    "    3. For the gini value of the right node, create a variable gini_right and use the gini function to calculate the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original gini impurity (without splitting!): 0.41848785606128835\n",
      "Left split gini impurity: 0.49485722848081015\n",
      "Right split gini impurity: 0.0\n"
     ]
    }
   ],
   "source": [
    "## 2. Gini impurity calculations\n",
    "gi = gini(y_train)\n",
    "left = y_train[x_train['persons_2'] == 0]\n",
    "right = y_train[x_train['persons_2'] == 1]\n",
    "gini_left = gini(left)\n",
    "gini_right = gini(right)\n",
    "\n",
    "print('Original gini impurity (without splitting!):', gi)\n",
    "print('Left split gini impurity:', gini_left)\n",
    "print('Right split gini impurity:', gini_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Before proceeding to calculate the information gain at this split, let’s consolidate what we’ve calculated in the previous checkpoints:\n",
    "\n",
    "    * There are 917 cars with a persons_2 value of 0 and 465 cars with persons_2 value of 1.\n",
    "    * The overall gini impurity of the training data is 0.4185. The gini impurity for the left split was 0.4949 and the gini impurity of the right split is 0.\n",
    "\n",
    "This means that the weighted impurity of this split is:\n",
    "917/1382 (0.4949) + 465/1382 (0) = 0.3284\n",
    "\n",
    "The information gain for tree whose root node is persons_2 should be\n",
    "0.4185 - 0.3284 =  0.0901\n",
    "\n",
    "Use the info_gain function to calculate the information gain corresponding to this split and store it as info_gain_persons_2. Print it to check if it is indeed the expected value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information gain for persons_2: 0.09013468781461476\n"
     ]
    }
   ],
   "source": [
    "## 3.Information gain when using feature `persons_2`\n",
    "info_gain_persons_2 = info_gain(left, right, gi)\n",
    "print(f'Information gain for persons_2:', info_gain_persons_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We’ve now verified that splitting at a root node of persons_2 gives us a lesser information gain than splitting at safety_low (0.0901 in comparison to 0.0918!). Verify the information gain is the highest at the root node using the function info_gain and looping through ALL the features. this calculation to verify if the tree we’ve been working with so far has the best possible root node!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greatest impurity gain at:0    safety_low\n",
      "1      0.091603\n",
      "Name: 19, dtype: object\n",
      "                 0         1\n",
      "19      safety_low  0.091603\n",
      "12       persons_2  0.090135\n",
      "18     safety_high  0.045116\n",
      "14    persons_more  0.025261\n",
      "13       persons_4  0.020254\n",
      "7      maint_vhigh  0.013622\n",
      "3     buying_vhigh  0.011001\n",
      "20      safety_med  0.008480\n",
      "17  lug_boot_small  0.006758\n",
      "1       buying_low  0.006519\n",
      "5        maint_low  0.005343\n",
      "6        maint_med  0.004197\n",
      "15    lug_boot_big  0.003913\n",
      "2       buying_med  0.003338\n",
      "8          doors_2  0.002021\n",
      "0      buying_high  0.001094\n",
      "4       maint_high  0.000530\n",
      "10         doors_4  0.000423\n",
      "16    lug_boot_med  0.000386\n",
      "11     doors_5more  0.000325\n",
      "9          doors_3  0.000036\n"
     ]
    }
   ],
   "source": [
    "## 4. Which feature split maximizes information gain?\n",
    "info_gain_list = []\n",
    "for i in x_train.columns:\n",
    "    left = y_train[x_train[i]==0]\n",
    "    right = y_train[x_train[i]==1]\n",
    "    info_gain_list.append([i, info_gain(left, right, gi)])\n",
    "\n",
    "info_gain_table = pd.DataFrame(info_gain_list).sort_values(1,ascending=False)\n",
    "print(f'Greatest impurity gain at:{info_gain_table.iloc[0,:]}')\n",
    "print(info_gain_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
