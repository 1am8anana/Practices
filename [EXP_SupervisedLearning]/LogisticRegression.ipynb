{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "Logistic regression is a supervised machine learning algorithm that predicts the probability, ranging from 0 to 1, of a datapoint belonging to a specific category, or class. These probabilities can then be used to assign, or classify, observations to the more probable group.\n",
    "\n",
    "For example, we could use a logistic regression model to predict the probability that an incoming email is spam. If that probability is greater than 0.5, we could automatically send it to a spam folder. This is called binary classification because there are only two groups (eg., spam or not spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "We saw that predicted outcomes from a linear regression model range from negative to positive infinity. These predictions don’t really make sense for a classification problem. Step in logistic regression!\n",
    "\n",
    "To build a logistic regression model, we apply a logit link function to the left-hand side of our linear regression function. Remember the equation for a linear model looks like this:\n",
    "$$\\\\y = b_{0}+m_{1}x_{1}+m_{2}x_{2}+...+m_{n}x_{n}$$\n",
    "When we apply the logit function, we get the following:\n",
    "$$\\\\ln(y / 1 - y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Log-Odds\n",
    "\n",
    "So far, we’ve learned that the equation for a logistic regression model looks like this:\n",
    "$$\\\\ln(p / 1 - p)$$\n",
    "Note that we’ve replaced y with the letter p because we are going to interpret it as a probability (eg., the probability of a student passing the exam). The whole left-hand side of this equation is called log-odds because it is the natural logarithm (ln) of odds (p/(1-p)). The right-hand side of this equation looks exactly like regular linear regression!\n",
    "\n",
    "In order to understand how this link function works, let’s dig into the interpretation of log-odds a little more. The odds of an event occurring is:\n",
    "$$\\\\Odds = \\frac{p}{1 - p} = \\frac{P(event occurring)}{P(event not occurring)}\\$$\n",
    "\n",
    "For example, suppose that the probability a student passes an exam is 0.7. That means the probability of failing is 1 - 0.7 = 0.3. Thus, the odds of passing are:\n",
    "$$\\\\Odds of passing = \\frac{0.7}{1 - 0.7} = 2.33$$\n",
    "This means that students are 2.33 times more likely to pass than to fail.\n",
    "\n",
    "Odds can only be a positive number. When we take the natural log of odds (the log odds), we transform the odds from a positive value to a number between negative and positive infinity — which is exactly what we need! The logit function (log odds) transforms a probability (which is a number between 0 and 1) into a continuous value that can be positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "\n",
    "Let’s return to the logistic regression equation and demonstrate how this works by fitting a model in sklearn. The equation is:\n",
    "$$\\\\ln(\\frac{p}{1-p}) = b_{0}+m_{1}x_{1}+m_{2}x_{2}+...+m_{n}x_{n}$$\n",
    "\n",
    "says if $$\\\\ln(\\frac{p}{1-p}) = output$$\n",
    "We can turn log odds into a probability as follows:\n",
    "$$\\\\p = \\frac{e^{output}}{1+e^{output}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculation that we just did required us to use something called the $$sigmoid function$$, which is the inverse of the logit function. The sigmoid function produces the S-shaped curve:\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a model in sklearn\n",
    "\n",
    "Now that we’ve learned a little bit about how logistic regression works, let’s fit a model using sklearn.\n",
    "\n",
    "To do this, we’ll begin by importing the LogisticRegression module and creating a LogisticRegression object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "# just example code don't run it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important note is that sklearn‘s logistic regression implementation requires the features to be standardized because regularization is implemented by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform features like (x1, x2, xn)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions in sklearn\n",
    "\n",
    "Using a trained model, we can predict whether new datapoints belong to the positive class (the group labeled as 1) using the .predict() method. The input is a matrix of features and the output is a vector of predicted labels, 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(features))\n",
    "# Sample output: [0 1 1 0 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are more interested in the predicted probability of group membership, we can use the .predict_proba() method. The input to predict_proba() is also a matrix of features and the output is an array of probabilities, ranging from 0 to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict_proba(features))\n",
    "# format output like [[proba of 0 is not occurr, proba of 1 occurring]]\n",
    "\n",
    "print(model.predict_proba(features)[:,1])\n",
    "# Sample output: [0.32 0.75  0.55 0.20 0.44]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Thresholding\n",
    "\n",
    "As we’ve seen, logistic regression is used to predict the probability of group membership. Once we have this probability, we need to make a decision about what class a datapoint belongs to. This is where the classification threshold comes in!\n",
    "\n",
    "The default threshold for sklearn is 0.5. If the predicted probability of an observation belonging to the positive class is greater than or equal to the threshold, 0.5, the datapoint is assigned to the positive class.\n",
    "\n",
    "We can choose to change the threshold of classification based on the use-case of our model. For example, if we are creating a logistic regression model that classifies whether or not an individual has cancer, we may want to be more sensitive to the positive cases. We wouldn’t want to tell someone they don’t have cancer when they actually do!\n",
    "\n",
    "In order to ensure that most patients with cancer are identified, we can move the classification threshold down to 0.3 or 0.4, increasing the sensitivity of our model to predicting a positive cancer classification. While this might result in more overall misclassifications, we are now missing fewer of the cases we are trying to detect: actual cancer patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix\n",
    "\n",
    "When we fit a machine learning model, we need some way to evaluate it. Often, we do this by splitting our data into training and test datasets. We use the training data to fit the model; then we use the test set to see how well the model performs with new data.\n",
    "\n",
    "As a first step, data scientists often look at a confusion matrix, which shows the number of true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "For example, suppose that the true and predicted classes for a logistic regression model are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0, 0, 1, 1, 1, 0, 0, 1, 0, 1]\n",
    "y_pred = [0, 1, 1, 0, 1, 0, 1, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a confusion matrix as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 2]\n",
      " [1 4]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output tells us that there are 3 true negatives, 1 false negative, 4 true positives, and 2 false positives. Ideally, we want the numbers on the main diagonal (in this case, 3 and 4, which are the true negatives and true positives, respectively) to be as large as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy, Recall, Precision, F1 Score\n",
    "\n",
    "Once we have a confusion matrix, there are a few different statistics we can use to summarize the four values in the matrix. These include accuracy, precision, recall, and F1 score. We won’t go into much detail about these metrics here, but a quick summary is shown below (T = true, F = false, P = positive, N = negative). For all of these metrics, a value closer to 1 is better and closer to 0 is worse.\n",
    "\n",
    "* Accuracy = (TP + TN)/(TP + FP + TN + FN)\n",
    "* Precision = TP/(TP + FP)\n",
    "* Recall = TP/(TP + FN)\n",
    "* F1 score: weighted average of precision and recall\n",
    "\n",
    "In sklearn, we can calculate these metrics as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.6666666666666666\n",
      "0.8\n",
      "0.7272727272727272\n"
     ]
    }
   ],
   "source": [
    "# accuracy:\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "# output: 0.7\n",
    "\n",
    "# precision:\n",
    "from sklearn.metrics import precision_score\n",
    "print(precision_score(y_true, y_pred))\n",
    "# output: 0.67\n",
    "\n",
    "# recall: \n",
    "from sklearn.metrics import recall_score\n",
    "print(recall_score(y_true, y_pred))\n",
    "# output: 0.8\n",
    "\n",
    "# F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_true, y_pred))\n",
    "# output: 0.73"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
